\chapter{Metriche}

In questo capitolo verranno illustrate delle tecniche 
per valutare un weighting model e dunque poterlo confrontare con gli altri.
\'E di necessaria importanza poterlo fare perché nel capitolo sull'ottimizzazione di BM25P dovremo avere un modo per eseguire
la cosidetta fase di \textit{model selection}. Dovremo cioè valutare se un modello con un iperparametro $\lambda$ è
migliore rispetto a un modello con un iperparametro $\lambda' \neq \lambda$.

Ma in generale per valutare le prestazioni di un motore di ricerca possiamo usare due unità di misura:

\begin{itemize}
	\item La \textbf{qualità}, intesa come l'effettività, cioè quanto il motore di ricerca
	è in grado di mostrare all'utente documenti che siano rilevanti con la query $q$.
	\item L'\textbf{efficienza}, cioè quanto tempo viene impiegato nel calcolo dei risultati, quindi in questo caso
	quanto tempo impiega la fase di retrieval.
\end{itemize}

L'obiettivo di questo tirocinio è stato quello di massimizzare la qualità di BM25P, pertanto
la seconda unità di misura non è rilevante, anche perché variando il valore di un iperparametro
non si hanno ripercussioni notevoli sull'efficienza, ma solo sulla qualità.

Per poter valutare l'effettività è necessario riprendere il concetto di rilevanza  che era stato
accennato nel capitolo 1. La rilevanza purtroppo è una misura che non è del tutto oggettiva,
poiché un utente potrebbe decidere che la query $q$ e il documento $d$ siano
in qualche modo collegati tra loro oppure no.

\begin{esempio}
	Sia $q$ la query \textit{``python"}. L'utente potrebbe intendere sia il linguaggio di programmazione Python
	che l'animale stesso. Dunque se questa query fosse sottomessa a un motore di ricerca
	esso potrebbe ritornare all'utente entrambi i risultati, che a seconda della necessità
	l'utente potrebbe valutarli come rilevanti oppure no.
\end{esempio}

Ciò che l'utente intende è chiamata \textit{Information Need}, cioè la richiesta di informazione
che si tenta di esprimere sottomettendo una query al sistema di ricerca.

La soggettività di questa valutazione è uno dei punti cruciali dell'Information Retrieval, per cui non esistono
metodi matematici per calcolarla, ed è necessario l'intervento di un essere umano.
Diamo dunque la definizione di rilevanza booleana:

\begin{definizione}\label{def:relb}
	La rilevanza booleana $\rel_B(d,q): \mathcal{D} \times \mathcal{Q} \rightarrow \mathbb{B}$
	è una funzione binaria che associa alla coppia query-documento un valore booleano
	true o false, in questo modo:
	\begin{itemize}
		\item		$\rel_B(d,q) = true$ se e solo se l'utente ritiene che il documento d è rilevante
		per la query $q$.
		\item $\rel_B(d,q) = false$ altrimenti.
	\end{itemize}
	
	Anche se ovvio, sottolineamo il fatto che $\rel_B(q,d)$ dipende unicamente dalla query q e
	dal documento $d$. In altri contesti questo concetto può esser chiamato anche con il nome
	di \textit{ground truth} oppure \textit{gold standard}.
\end{definizione}

Il valore di tale funzione deve sempre esser calcolato da un essere umano, poiché
deve essere, come da definizione, deve rappresentare una verità soggettiva.

Ma ciò potrebbe sembrare però un po' limitante, poiché quando si parla di scoring,
$\sco(q,d)$ restituisce valori continui in $\mathbb{R}$ e dunque come può
approssimare $\rel_B$?
La risposta sta nel fatto che un ranker $\mathcal{R}$ si limita ad ordinare i documenti utilizzando come peso
il valore attribuito al documento dalla funzione $\sco$. Pertanto se ipotizzassimo di prendere
i primi $k$ elementi dalla lista $\mathcal{L}$, restituita dal motore di ricerca, potremo affermare che:

\begin{itemize}
	\item Se $d \in \mathcal{L}$  allora $\rel_{\mathcal{R}}(d, q) = true$
	\item Se $d \notin \mathcal{L}$ allora $\rel_{\mathcal{R}}(d,q) = false$
\end{itemize}

Attraverso questa definizione siamo in grado dunque di definire le modalità di valutazione di
un motore di ricerca, poiché possiamo paragonare i risultati di $\rel_B$ con quelli di $\rel_{\mathcal{R}}$,
ottenendo dunque un meccanismo di comparazione tra i risultati attesi e i risultati calcolati.

\subsection{Il principio PRP}

Introduciamo in questo paragrafo il \textit{Probability Ranking Principle}, ovvero un risultato
teorico che ci consente di avere una base matematica per valutare l'ottimalità di un ranker.
Tale principio afferma che:

\begin{definizione}\label{def:prp}
	Se per ogni query $q \in \mathcal{Q}$ il sistema ordina i documenti della collezione $\mathcal{D}$ in
	senso decrescente rispetto alla probabilità di rilevanza, allora l'effettività complessiva del sistema
	è la migliore ottenibile con i dati a disposizione.
\end{definizione}

Questo principio rappresenta l'unico metodo formale per verificare l'effettività di un sistema
di ricerca, poichè migliore è la stima che possiamo calcolare della rilevanza, migliori sono
le prestazioni.
Per completezza della trattazione dell'argomento diamo
una illustrazione di come dimostrare questo principio.

\begin{proof}
	Iniziamo la dimostrazione procedendo per punti:
	\begin{itemize}
		\item Sia $\mathcal{R}$ un ranker
		\item Siano $\mathcal{Q}$ e $\mathcal{D}$ gli insiemi delle query e dei documenti
		\item In questo contesto, sia $\rel_\mathcal{R}$ una variabile con natura aleatoria
		\item $\forall{q \in \mathcal{Q}, d \in \mathcal{D}}$ sia $P(\rel_B = r  | D = d, Q = q )$
		la probabilità che il documento $d$ sia rilevante per $q$.
		\item Sia $\mathcal{L}$  la lista ordinata dei documenti ritornati dal motore di ricerca, di lunghezza arbitraria
		$k$.
	\end{itemize}
	
	Il valore atteso della rilevanza possiamo calcolarlo come:
	\begin{align*}
	& \mathbb{E}\left[\rel_B\right] = \sum_{i=1}^{k}{
		1 \cdot P(\rel_B = true  | D = d_i, Q = q ) + \cancelto{0}{0 \cdot  P(\rel_B = false  | D = d_i, Q = q )}
	} \\
	& \mathbb{E}\left[\rel_B\right] = \sum_{i=1}^{k}{P(\rel_B = true  | D = d_i, Q = q )}
	\end{align*}
	
	Ciò che vogliamo dimostrare è che per ogni valore di $k>0$, il valore atteso è massimo quando i documenti
	sono ordinati in senso decrescente.
	Procediamo dunque per assurdo ipotizzando esista un valore $\tilde{k}$ tale che $\mathbb{E}[\rel_B]$ sia
	massimo anche se i documenti non sono ordinati in tal modo.
	Si verifica dunque che esiste almeno un documento $d_l$ che non viene inserito in $\mathcal{L}$ nell'ordinamento
	decrescente e inoltre ciò impone che un altro documento $d_m$ non venga inserito nell'ultima posizione della lista.
	Abbiamo quindi che:
	$$
	P(\rel_B = r | D = d_l, Q=q)  < P(\rel_B = r| D = d_m, Q = q)
	$$
	
	e dunque:
	
	\begin{align*}
	& \mathbb{E}\left[\rel_B\right]_{\tilde{k}} = \sum_{i=1}^{\tilde{k}}{P(\rel_B = true | D = d_i, Q = q)} \\
	& \mathbb{E}[\rel_B]_{\tilde{k}} \Big|_{d_{\tilde{k}} = d_{l}} < \mathbb{E}[\rel_B]_{\tilde{k}} \Big|_{d_{\tilde{k}} = d_{m}}
	\end{align*}
	
	Ciò però è assurdo, perché per ipotesi $\mathbb{E}\left[\rel_B\right]_{\tilde{k}}$ sostituendo $d_{\tilde{k}}$ con $d_l$ doveva
	essere massimo.
\end{proof}

\section{Recall e Precision}

Adesso che abbiamo raccolto tutte le informazioni necessarie possiamo definire
le unità di misura per la qualità di un motore di ricerca.
Le misure più semplici che sono adottate attualmente sono  la recall e la precision.

\begin{definizione}\label{def:recall}
	La recall, indicata con il simbolo $\re$ è definita come il rapporto tra il numero di documenti
	messi in lista da $\mathcal{R}$, per cui $\rel_B(d,q) = true$,
	e il numero di elementi totali giudicati rilevanti.
	In simboli si ha:
	
	$$
	\re = P(d \in \mathcal{L} | \rel_B(d,q) = true) = \frac{
		\#\left\{ d \in \mathcal{L} | \rel_B(d,q) = true\right\}
	}{
		\#\left\{ d \in \mathcal{D} | \rel_B(d,q) = true\right\}
	}
	$$
\end{definizione}

Ovviamente affinchè la recall sia massima, il ranker dovrebbe ritornare la lista di tutti i documenti
rilevanti, ma questo non sempre risulta possibile, poiché $\left|\mathcal{L}\right| \lll \left|\mathcal{D}\right|$.

\begin{definizione}\label{def:precision}
	La precision, indicata con il simbolo $\preci$ è uguale alla recall
	ma con la differenza che il rapporto non ha come denominatore tutti i documenti
	rilevanti, ma solo quelli in lista.
	
	$$
	\re = P(d \in \mathcal{L} | \rel_B(d,q) = true) = \frac{
		\#\left\{ d \in \mathcal{L} | \rel_B(d,q) = true\right\}
	}{
		\left|\mathcal{L}\right|
	}
	$$
\end{definizione}
Invece per precision, raggiungere il valore massimo è abbastanza facile, specialmente quando $\left|\mathcal{L}\right| \approx 1$.

\pagebreak

Un modo analogo per calcolare $\re$ e $\preci$ è quello di creare una sorta di confusion
matrix, nel seguente modo:

\begin{table}[h!]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		& $\rel_b = true$ & $\rel_b = false$ \\
		\hline
		$d \in \mathcal{L}$ &  vero positivo (tp) & falso positivo (fp) \\
		\hline
		$d \notin \mathcal{L}$ &  falso negativo (fn) & vero negativo (tn) \\
		\hline
	\end{tabular}
	\caption{Matrice di contingenza, o matrice di confusione}
\end{table}

Una volta avvolarata la matrice, si può calcolare $\preci = \frac{tp}{tp + fp}$ e $\re = \frac{tp}{tp + fn}$.

\paragraph{Il problema delle misure}
Il problema principale di queste due misure è il fatto che statisticamente il $99.9\%$ dei documenti
risulta non rilevante per una query $q$. 
Infatti se la funzione obiettivo da massimizzare fosse recall
un sistema che marca tutti i documenti come non rilevanti avrebbe
un valore molto vicino ad un sistema che invece funziona bene. Inoltre
queste misure sono basate sull'insieme dei documenti restituiti e non tengono
conto dell'ordine della lista $\mathcal{L}$.

Per esempio un utente che naviga nel web è interessato
ad avere come primo risultato esattamente il documento che stava cercando,
e non vorrebbe scorrere i risultati fino a trovare quello che stava cercando.
Precision e recall quindi sono misure semplici ed effettive, ma non sono
complete per la valutazione di un motore di ricerca.

\section{NDCG}
L'unità di misura che andremo a considerare è la Normative Discontinued Cumulative Gain,
definita come segue:

$$\label{def:ndcg}
NDCG(O, k) = \frac{1}{\left|O\right|} \sum_{q \in O} Z_{k, q} \cdot \sum_{i=1}^{k} \frac{2^{R(d_i, q)} - 1}{\log_2{(i + 1)}} \\
$$

in cui abbiamo che:
\begin{table}[h!]
	\centering
	\begin{tabular}{|c|c|}
		\hline
		$ O \subset Q$ & è un sottoinsieme di query \\
		\hline
		$Z_{k,q}$ & è il fattore di normalizzazione \\
		\hline
		$R(d,q)$ & è il gold standard, possiamo assumerla uguale a $\rel_B$ \\
		\hline
	\end{tabular}
\end{table}


Come è possibile notare dalla formula, in questo modo andiamo a considerare un sottoinsieme delle query, che chiamiamo $O$,
e un intero $k>1$ che conta il numero di documenti nella lista $\mathcal{L}$.
Espandendo la seconda sommatoria $s_2$ è interessante notare che:
$$
s_2 = \frac{2^{R(d_1, q) } - 1}{1} + \frac{2^{R(d_2, q)} - 1}{1.58} + \frac{2^{R(d_3, q)} - 1}{2} + \dotsb + \frac{2^{R(d_k, q) } - 1}{\log_2{k + 1}}
$$

Siccome la funzione $log_2(x)$ è monotona crescente, $s_2$ è costruita in modo tale da
premiare maggiormente i risultati con $k$ piccolo e penalizzare quelli con $k$ più grande.
Parlando anche in termini computazionali, calcolare il valore dell'esponente al numeratore
non è troppo dispendioso poiché:
\begin{enumerate}
	\item La base dell'esponente è 2 e il valore di $R(d,q)$ è intero, dunque si può fare bit shifting.
	\item La funzione $R(d,q)$ inoltre, se la assumiamo uguale a $\rel_B$ assume solo due valori,
	o anche se fosse diversa essa assumerà un insieme finito di valori che può essere precomputato.
\end{enumerate}

In questo modo abbiamo ottenuto una misura di valutazione che è in grado di tenere in considerazione
sia del problema della quantità di dati, sia dell'ordinamento dei risultati.
\footnote{Magari qui posso far vedere qualche grafico oppure parlare meglio del fattore di normalizzazione $Z_{i,j}$}
\footnote{Dovrei parlare anche dei tagli di recall e precision?}
\footnote{Dovrei descrivere con qualche riga i dataset che sono a disposizione? aquaint, RCV, ...?}